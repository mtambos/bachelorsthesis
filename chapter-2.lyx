#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass scrbook
\begin_preamble
% increases link area for cross-references and autoname them
% if you change the document language to e.g. French
% you must change "extrasenglish" to "extrasfrench"
\AtBeginDocument{%
 \renewcommand{\ref}[1]{\mbox{\autoref{#1}}}
}
\def\refnamechanges{%
 \renewcommand*{\equationautorefname}[1]{}
 \renewcommand{\sectionautorefname}{sec.\negthinspace}
 \renewcommand{\subsectionautorefname}{sec.\negthinspace}
 \renewcommand{\subsubsectionautorefname}{sec.\negthinspace}
 \renewcommand{\figureautorefname}{Fig.\negthinspace}
 \renewcommand{\tableautorefname}{Tab.\negthinspace}
}
\@ifpackageloaded{babel}{\addto\extrasenglish{\refnamechanges}}{\refnamechanges}

% in case somebody want to have the label "Equation"
%\renewcommand{\eqref}[1]{Equation~(\negthinspace\autoref{#1})}

% that links to image floats jumps to the beginning
% of the float and not to its caption
\usepackage[figure]{hypcap}

% the pages of the TOC is numbered roman
% and a pdf-bookmark for the TOC is added
\let\myTOC\tableofcontents
\renewcommand\tableofcontents{%
  \frontmatter
  \pdfbookmark[1]{\contentsname}{}
  \myTOC
  \mainmatter }

% makes caption labels bold
% for more info about these settings, see
% http://mirrors.ctan.org/macros/latex/contrib/koma-script/doc/scrguien.pdf
\setkomafont{captionlabel}{\bfseries}
\setcapindent{1em}

% enables calculations
\usepackage{calc}

% fancy page header/footer settings
% for more information see section 9 of
% ftp://www.ctan.org/pub/tex-archive/macros/latex2e/contrib/fancyhdr/fancyhdr.pdf
\renewcommand{\chaptermark}[1]{\markboth{#1}{#1}}
\renewcommand{\sectionmark}[1]{\markright{\thesection\ #1}}

% increases the bottom float placement fraction
\renewcommand{\bottomfraction}{0.5}

% avoids that floats are placed above its sections
\let\mySection\section\renewcommand{\section}{\suppressfloats[t]\mySection}
\end_preamble
\options intoc,bibliography=totoc,index=totoc,BCOR10mm,captions=tableheading,titlepage,fleqn
\use_default_options true
\master Online Anomaly Detection over Time Series using Merge Growing Neural Gas.lyx
\begin_modules
customHeadersFooters
theorems-ams-bytype
theorems-ams-extended-bytype
theorems-sec-bytype
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman lmodern
\font_sans lmss
\font_typewriter lmtt
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\float_placement h
\paperfontsize 12
\spacing single
\use_hyperref true
\pdf_title "Your title"
\pdf_author "Your name"
\pdf_bookmarks true
\pdf_bookmarksnumbered true
\pdf_bookmarksopen true
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle false
\pdf_quoted_options "pdfpagelayout=OneColumn, pdfnewwindow=true, pdfstartview=XYZ, plainpages=false"
\papersize a4paper
\use_geometry false
\use_amsmath 2
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 0
\branch NoChildDocument
\selected 0
\filename_suffix 0
\color #ff0000
\end_branch
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 2
\paragraph_separation skip
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 2
\paperpagestyle fancy
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Left Header
\begin_inset Argument
status open

\begin_layout Plain Layout
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
chaptername
\end_layout

\end_inset


\begin_inset space ~
\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
thechapter
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
rightmark
\end_layout

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Enable page headers and add the chapter to the header line.
\end_layout

\end_inset


\end_layout

\begin_layout Right Header
\begin_inset Argument
status open

\begin_layout Plain Layout
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
leftmark
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Left Footer
\begin_inset Argument
status open

\begin_layout Plain Layout
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
thepage
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Center Footer

\end_layout

\begin_layout Right Footer
\begin_inset Argument
status open

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
thepage
\end_layout

\end_inset


\end_layout

\begin_layout Chapter
Merge Growing Neural Gas for Anomaly Detection
\begin_inset CommandInset label
LatexCommand label
name "chap:2"

\end_inset


\end_layout

\begin_layout Section
Method Intuition
\begin_inset CommandInset label
LatexCommand label
name "sec:2.1"

\end_inset


\end_layout

\begin_layout Standard
Our take on on-line anomaly detection over time series will use the following
 assumption: if we take a continuous sample of a time series, that sample
 will be considered normal if it is similar to other segments of the time
 series, the more similar it is, the more normal it is, and conversely,
 the more dissimilar the sample is to other segments, the more abnormal
 the sample is.
\end_layout

\begin_layout Standard
Since we don't know a priori how long the series will be, nor want we to
 make the computation depend on the length of the series, we have first
 taken the idea from 
\begin_inset CommandInset citation
LatexCommand cite
key "Wei2005"

\end_inset

 of using two sliding windows: for a time series 
\begin_inset Formula $S=(\mathbf{a}_{0},...,\mathbf{a}_{n}),\mathbf{a}_{i}\in\mathbb{R}^{m},0\le i\le n,m>1$
\end_inset

, a window 
\begin_inset Formula $P=(\mathbf{a}_{i},....,\mathbf{a}_{j}),0\le i,j<n$
\end_inset

 represents the past, and a second window 
\begin_inset Formula $Q=(\mathbf{a}_{j+1},...,\mathbf{a}_{n})$
\end_inset

 represents the present; one notes that the two windows are contiguous.
 What we want then, is to train two separate models: 
\begin_inset Formula $M_{P}$
\end_inset

 for the past window and 
\begin_inset Formula $M_{Q}$
\end_inset

 for the present window.
\end_layout

\begin_layout Standard
The approach we use to constructing 
\begin_inset Formula $M_{P}$
\end_inset

and 
\begin_inset Formula $M_{Q}$
\end_inset

 is called Merge Growing Neural Gas, MGNG for short.
 One of MGNG's properties is that it builds a fixed size representation
 of the input space, i.e.
 if we for instance set the representation to be size 100, it will continue
 to be 100 even after we have fed it 1000 or 10000 data points.
 This allows us to set 
\begin_inset Formula $P=(\mathbf{a}_{0},....,\mathbf{a}_{j})$
\end_inset

, that is to consider our past window as all data points until time step
 
\begin_inset Formula $j$
\end_inset

.
\end_layout

\begin_layout Standard
Given a size 
\begin_inset Formula $k$
\end_inset

 for the 
\emph on
present
\emph default
 window, at each time step 
\begin_inset Formula $t$
\end_inset

 we do the following:
\end_layout

\begin_layout Enumerate
We feed data point 
\begin_inset Formula $\mathbf{a}_{t}$
\end_inset

 to 
\begin_inset Formula $M_{Q}$
\end_inset

 and data point 
\begin_inset Formula $\mathbf{a}_{t-k}$
\end_inset

 to 
\begin_inset Formula $M_{P}$
\end_inset

;
\end_layout

\begin_layout Enumerate
then we check how close 
\begin_inset Formula $M_{Q}$
\end_inset

 is to 
\begin_inset Formula $M_{P}$
\end_inset

, by means of computing a distance metric 
\begin_inset Formula $D_{t}$
\end_inset

 (some choices for 
\begin_inset Formula $D_{t}$
\end_inset

 shall be explained later);
\end_layout

\begin_layout Enumerate
finally, based on 
\begin_inset Formula $D_{t}$
\end_inset

, we calculate and return an 
\begin_inset Quotes eld
\end_inset

anomaly-level
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $A_{t}$
\end_inset

.
\end_layout

\begin_layout Standard
Now that we have an idea of 
\emph on
what
\emph default
 we want to do, we need to go into the 
\emph on
how
\emph default
.
 The first step is to explain how the underlying method for time series
 representation, MGNG, works.
\end_layout

\begin_layout Section
Merge Growing Neural Gas
\begin_inset CommandInset label
LatexCommand label
name "sec:2.2"

\end_inset


\end_layout

\begin_layout Verse

\emph on
Based on 
\begin_inset CommandInset citation
LatexCommand cite
key "Andreakis2009"

\end_inset

.
\end_layout

\begin_layout Standard
Merge Growing Neural Gas (MGNG) is thought as an extension to Growing Neural
 Gas (GNG, 
\begin_inset CommandInset citation
LatexCommand cite
key "Fritzke1995"

\end_inset

), in turn a type of Self Organizing Network capable of adding and deleting
 nodes to adapt to the input space.
 Both GNG and MGNG learn the input space's topology, however MGNG is also
 capable of learning the 
\emph on
context
\emph default
 associated with each neuron, i.e.
 what the data points that occurred before the selection of the neuron look
 like.
\end_layout

\begin_layout Standard
A MGNG network consists of a set of 
\begin_inset Formula $\mathcal{K}$
\end_inset

 neurons connected by a set of edges 
\begin_inset Formula $\mathcal{E}$
\end_inset

.
 All 
\begin_inset Formula $n\in\mathcal{K}$
\end_inset

 have two weight vectors 
\begin_inset Formula $\mathbf{w}_{n}$
\end_inset

 and 
\begin_inset Formula $\mathbf{c}_{n}$
\end_inset

, both of the same dimensionality as the input space, plus an counter 
\begin_inset Formula $e_{n}$
\end_inset

.
\end_layout

\begin_layout Standard
An input sequence 
\begin_inset Formula $\left(\mathbf{a}_{1},...,\mathbf{a}_{t}\right)$
\end_inset

 is assigned to the best matching neuron 
\begin_inset Formula $r$
\end_inset

 (also called winner or BMU) by finding the neuron 
\begin_inset Formula $n$
\end_inset

 with lowest distance 
\begin_inset Formula $d_{n}$
\end_inset

:
\begin_inset Formula 
\begin{equation}
d_{n}(t)\coloneqq(1-\alpha)\cdot\left\Vert \mathbf{a}_{t}-\mathbf{w}_{n}\right\Vert ^{2}+\alpha\cdot\left\Vert \mathbf{C}_{t}-\mathbf{c}_{n}\right\Vert ^{2}\label{eq:1}
\end{equation}

\end_inset

In this context 
\begin_inset Formula $\mathbf{w}_{n}$
\end_inset

 approximates the data point at the current time step and
\begin_inset Formula $\mathbf{c}_{n}$
\end_inset

 represents all previous data points, while 
\begin_inset Formula $e_{n}$
\end_inset

 keeps track of how many times 
\begin_inset Formula $n$
\end_inset

 was the BMU.
 The parameter 
\begin_inset Formula $\alpha\in[0,1]$
\end_inset

 weights the importance of the current input signal over the past.
 
\begin_inset Formula $\mathbf{C}_{t}$
\end_inset

 is called the global temporal context.
 
\begin_inset Formula $\mathbf{C}_{t+1}$
\end_inset

, i.e.
 the global temporal context of the next time step, is computed as a linear
 combination (merge) of the weight and context vector from 
\begin_inset Formula $r$
\end_inset

 
\begin_inset Formula 
\begin{equation}
\mathbf{C}_{t+1}\coloneqq(1-\beta)\cdot\mathbf{w}_{r}+\beta\cdot\mathbf{c}_{r}\label{eq:2}
\end{equation}

\end_inset

The parameter 
\begin_inset Formula $\beta\in[0,1]$
\end_inset

 controls the influence of the far over the recent past.
 The temporal context 
\begin_inset Formula $\mathbf{C}_{t}$
\end_inset

 constitutes an exponentially decayed sum of all past winnerâ€™s weight vectors
 beside the current.
 Hebbian learning takes place by adapting 
\begin_inset Formula $r$
\end_inset

 and its direct topological neighbors 
\begin_inset Formula $\mathcal{N}_{r}$
\end_inset

 towards the recent input signal 
\begin_inset Formula $\mathbf{a}_{t}$
\end_inset

 and the past 
\begin_inset Formula $\mathbf{C}_{t}$
\end_inset

 for given learning rates 
\begin_inset Formula $\epsilon_{w}$
\end_inset

 and 
\begin_inset Formula $\epsilon_{n}$
\end_inset

 as follows:
\begin_inset Formula 
\begin{align*}
\mathbf{w}_{r}\coloneqq & \mathbf{w}_{r}+\epsilon_{w}\cdot\left(\mathbf{a}_{t}-\mathbf{w}_{r}\right)\\
\mathbf{c}_{r}\coloneqq & \mathbf{c}_{r}+\epsilon_{w}\cdot\left(\mathbf{C}_{t}-\mathbf{c}_{r}\right)
\end{align*}

\end_inset

and 
\begin_inset Formula $\forall n\in\mathcal{N}_{r}$
\end_inset

:
\begin_inset Formula 
\begin{align*}
\mathbf{w}_{n}\coloneqq & \mathbf{w}_{n}+\epsilon_{n}\cdot\left(\mathbf{a}_{t}-\mathbf{w}_{n}\right)\\
\mathbf{c}_{n}\coloneqq & \mathbf{c}_{n}+\epsilon_{n}\cdot\left(\mathbf{C}_{t}-\mathbf{c}_{n}\right)
\end{align*}

\end_inset

Additionally, the counter 
\begin_inset Formula $e_{r}$
\end_inset

 is increased: 
\begin_inset Formula $e_{r}\coloneqq e_{r}+1$
\end_inset

.
\end_layout

\begin_layout Standard
The connection between 
\begin_inset Formula $r$
\end_inset

 and second best matching unit 
\begin_inset Formula $s$
\end_inset

 is created or refreshed following a competitive Hebbian learning approach
 the following way:
\begin_inset Formula 
\begin{align*}
\mathcal{E}\coloneqq & \mathcal{E}\cup\left\{ \left(r,s\right)\right\} \\
age_{\left(r,s\right)}\coloneqq & 0
\end{align*}

\end_inset

All other connections are weakened and too infrequent connections are deleted
 depending on the parameter 
\begin_inset Formula $\gamma$
\end_inset

:
\begin_inset Formula 
\begin{align*}
\forall n\in\mathcal{N}_{r},age_{\left(r,n\right)}\coloneqq & age_{\left(r,n\right)}+1\\
\mathcal{E}\coloneqq & \mathcal{E}\setminus\left\{ \left(a,b\right)\vert age_{\left(r,b\right)}>\gamma\right\} 
\end{align*}

\end_inset

The network grows at regular time intervals 
\begin_inset Formula $\lambda$
\end_inset

 up to a maximal size 
\begin_inset Formula $\theta$
\end_inset

 by insertion of new nodes based on entropy maximization:
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
\begin_inset Formula $\textbf{if}\,\left(t\,\mod\,\lambda\cong0\right)\wedge\left(\left|\mathcal{K}\right|<\theta\right)\,\textbf{then:}$
\end_inset

 
\end_layout

\begin_deeper
\begin_layout Enumerate
find neuron 
\begin_inset Formula $q$
\end_inset

 with the greatest counter: 
\begin_inset Formula $q\coloneqq\underset{\left(n\in\mathcal{K}\right)}{\arg\max}e_{n}$
\end_inset


\end_layout

\begin_layout Enumerate
find neighbor 
\begin_inset Formula $f$
\end_inset

 of 
\begin_inset Formula $q$
\end_inset

 with 
\begin_inset Formula $f\coloneqq\underset{\left(n\in\mathcal{N}_{q}\right)}{\arg\max}e_{n}$
\end_inset


\end_layout

\begin_layout Enumerate
initialize new node 
\begin_inset Formula $l$
\end_inset

:
\begin_inset Formula 
\begin{align*}
\mathcal{K}\coloneqq & \mathcal{K}\cup\left\{ l\right\} \\
\mathbf{w}_{l}\coloneqq & \frac{1}{2}\cdot(\mathbf{w}_{q}+\mathbf{w}_{f})\\
\mathbf{c}_{l}\coloneqq & \frac{1}{2}\cdot(\mathbf{c}_{q}+\mathbf{c}_{f})\\
e_{l}\coloneqq & \delta\cdot(e_{f}+e_{q})
\end{align*}

\end_inset


\end_layout

\begin_layout Enumerate
adapt connections: 
\begin_inset Formula $\mathcal{E}\coloneqq\left(\mathcal{E}\setminus\left\{ \left(q,f\right)\right\} \right)\cup\left\{ \left(q,l\right),\left(l,f\right)\right\} $
\end_inset


\end_layout

\begin_layout Enumerate
decrease counters 
\begin_inset Formula $e_{q}$
\end_inset

 and 
\begin_inset Formula $e_{f}$
\end_inset

 by the factor 
\begin_inset Formula $\delta$
\end_inset


\begin_inset Formula 
\begin{align*}
e_{q}\coloneqq & (1-\delta)\cdot e_{q}\\
e_{f}\coloneqq & (1-\delta)\cdot e_{f}
\end{align*}

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Marginal
status collapsed

\begin_layout Plain Layout
Add a graphic showing how a network with 3 or 4 nodes, and how it behaves
 when it's fed 5-10 data points in 
\begin_inset Formula $\mathbb{R}^{1}$
\end_inset

.
\end_layout

\end_inset

Now that we have an idea of the inner workings of MGNG, we go into how to
 use it to perform anomaly detection.
\end_layout

\begin_layout Section
MGNG for Anomaly Detection
\begin_inset CommandInset label
LatexCommand label
name "sec:2.3"

\end_inset


\end_layout

\begin_layout Standard
In 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:2.1"

\end_inset

 we explained roughly in which manner we want to use MGNG for detecting
 anomalies in time series, now we would like to dive into the details.
 Our algorithm is described below:
\end_layout

\begin_layout Algorithm
\begin_inset space ~
\end_inset


\begin_inset CommandInset label
LatexCommand label
name "alg:2.3.1"

\end_inset


\begin_inset Marginal
status open

\begin_layout Plain Layout
Maybe I should also add an example of how it works step-by-step over a small
 series in 
\begin_inset Formula $\mathbb{R}^{1}$
\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Define the present window size 
\begin_inset Formula $j$
\end_inset

.
\end_layout

\begin_layout Enumerate
Initialize 
\begin_inset Formula $M_{P}$
\end_inset

 and 
\begin_inset Formula $M_{Q}$
\end_inset

.
 In this work we choose to initialize both models with exactly the same
 parameters, another option would be to allow 
\begin_inset Formula $M_{P}$
\end_inset

 to grow a bigger network than 
\begin_inset Formula $M_{Q}$
\end_inset

 (we will not cover this option here).
 The maximum sizes 
\begin_inset Formula $\theta_{P}$
\end_inset

 and 
\begin_inset Formula $\theta_{Q}$
\end_inset

 should be chosen so to allow the models to capture the time series with
 sufficient detail.
\end_layout

\begin_layout Enumerate
Create a buffer 
\begin_inset Formula $B$
\end_inset

 of size 
\begin_inset Formula $j$
\end_inset

.
\end_layout

\begin_layout Enumerate
In each time step 
\begin_inset Formula $t$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Enumerate
Append the data point 
\begin_inset Formula $\mathbf{a}_{t}$
\end_inset

 to the end of 
\begin_inset Formula $B$
\end_inset

.
\end_layout

\begin_layout Enumerate
Feed 
\begin_inset Formula $\mathbf{a}_{t}$
\end_inset

 to 
\begin_inset Formula $M_{Q}$
\end_inset

.
\end_layout

\begin_layout Enumerate
If 
\begin_inset Formula $t\ge j$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Enumerate
Retrieve the oldest element 
\begin_inset Formula $\mathbf{a}_{o}$
\end_inset

 from B.
\end_layout

\begin_layout Enumerate
Feed 
\begin_inset Formula $\mathbf{a}_{o}$
\end_inset

 to 
\begin_inset Formula $M_{P}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Calculate the distance metric 
\begin_inset Formula $D_{t}$
\end_inset


\end_layout

\begin_layout Enumerate
Calculate the anomaly level 
\begin_inset Formula $A_{t}\coloneqq\alpha\cdot A_{t-1}+D_{t}$
\end_inset


\end_layout

\end_deeper
\end_deeper
\end_deeper
\begin_layout Standard
The two critical choices here are how to calculate both 
\begin_inset Formula $D_{t}$
\end_inset

 and 
\begin_inset Formula $A_{t}$
\end_inset

.
 We propose four methods of computing 
\begin_inset Formula $D_{t}$
\end_inset

 that are going to be compared later in our experiments in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "chap:3"

\end_inset

.
\end_layout

\begin_layout Standard
Three of the use the same distance, namely:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
D_{t}\coloneqq\sum_{n\in\mathcal{K}_{Q}}(1-\omega)\cdot\left\Vert \mathbf{w}_{n}-\mathbf{w}_{f\left(n,\omega\right)}\right\Vert ^{2}+\omega\cdot\left\Vert \mathbf{c}_{n}-\mathbf{c}_{f\left(n,\omega\right)}\right\Vert ^{2},0\le\omega\le1,f\left(n,\omega\right)\in\mathcal{K}_{P}
\]

\end_inset

but differ on how the define 
\begin_inset Formula $f\left(n,\omega\right)$
\end_inset

:
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula 
\begin{align*}
f\left(n,\omega\right)\coloneqq & \underset{k}{\arg\min}\left\Vert \mathbf{w}_{n}-\mathbf{w}_{k}\right\Vert \\
f\left(n,\omega\right)\coloneqq & \underset{k}{\arg\min}\left\Vert \mathbf{c}_{n}-\mathbf{c}_{k}\right\Vert \\
f\left(n,\omega\right)\coloneqq & \underset{k}{\arg\min}\omega\cdot\left\Vert \mathbf{w}_{n}-\mathbf{w}_{f\left(n\right)}\right\Vert ^{2}+\omega\cdot\left\Vert \mathbf{c}_{n}-\mathbf{c}_{f\left(n\right)}\right\Vert ^{2}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The fourth option is to measure the temporal quantization error (
\begin_inset CommandInset citation
LatexCommand cite
key "Voegtlin2002"

\end_inset

)
\begin_inset Marginal
status collapsed

\begin_layout Plain Layout
Here I should put the explicit formula, but I don't understand it yet.
\end_layout

\end_inset

.
\end_layout

\end_body
\end_document
