#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass book
\begin_preamble
% Added by lyx2lyx
% Added by lyx2lyx
\end_preamble
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package none
\inputencoding utf8
\fontencoding default
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 11
\spacing single
\use_hyperref false
\papersize a4paper
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 2
\use_package mhchem 0
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Stichwortverzeichnis
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 2
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Section
Overview
\begin_inset CommandInset label
LatexCommand label
name "sec:Overview"

\end_inset


\end_layout

\begin_layout Standard
This section starts with an introduction to the HTM, in order to better
 understand how it is similar to MGNG-AD and why it behaves the way it does
 in the experiments.
 Then a brief analysis of the influence of how the different parameters
 in a MGNG model impact on the anomaly detection capabilities of a MGNG-AD
 model will be given.
 Finally the experiment's setup and evaluation methodology will be explained.
\end_layout

\begin_layout Subsection
Overview of HTM, the CLA and the OPF
\begin_inset CommandInset label
LatexCommand label
name "sub:Overview_of_HTM"

\end_inset


\end_layout

\begin_layout Verse

\emph on
The present section is based on the work by Hawkins et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Hawkins2011"

\end_inset

.
\end_layout

\begin_layout Standard
HTMs can be viewed as a type of neural network.
 In it neurons are organized in a two dimensional array of columns; each
 column builds an array of neurons itself.
 An HTM models its inputs by activating or deactivating its neurons, such
 representation of information is called as Sparse Distributed Representation
 (SDR
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "SDR"
description "Sparse Distributed Representation"

\end_inset

).
 The representation is sparse because at any given moment only a small fraction
 of neurons are active; it is called distributed because the activation
 of a single neuron carries little meaning -- many neurons need to be simultaneo
usly activated to represent a complete input.
\end_layout

\begin_layout Standard
An HTM is organized in a hierarchy of regions, where each region uses a
 SDR and represents a level in the hierarchy.
 The communication in the hierarchy follows three routes: forward communication
 from lower regions to higher regions, feedback communication from higher
 regions to lower regions, and lateral communication in the form of excitation/i
nhibition among cells or columns in the same region.
 In the hierarchy, lower levels learn basic concepts, like edges and corners
 in an image, while higher levels learn more abstract concepts, like 
\begin_inset Quotes eld
\end_inset

car
\begin_inset Quotes erd
\end_inset

 or 
\begin_inset Quotes eld
\end_inset

flower
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Standard
HTMs perform four basic functions: learning, inference, prediction and behavior.
 Of interest here are however only the first three.
 Each of these functions are performed not by the HTM as a whole, but in
 each region by itself.
\end_layout

\begin_layout Standard
During 
\emph on
learning
\emph default
 a region tries to build an internal representation of its inputs.
 Its goal is to find patterns in each input signal, called 
\emph on
spatial patterns
\emph default

\begin_inset Foot
status collapsed

\begin_layout Plain Layout
This is akin to an offline clustering task.
\end_layout

\end_inset

, as well as patterns in sequences of input signals, called 
\emph on
temporal patterns
\emph default
.
\end_layout

\begin_layout Standard
A region uses what it has learned to perform 
\emph on
inference
\emph default
 by trying to match new inputs and sequences of new inputs to the patterns
 it has already stored.
\end_layout

\begin_layout Standard
The final task discussed here is 
\emph on
prediction
\emph default
.
 In it, a region tries to guess what input signals could come next, based
 on the spatio-temporal patterns it has learned and the input signals seen
 so far.
 In the context of anomaly detection, this prediction is used to calculate
 an anomaly score as a difference between what was predicted and what was
 in fact seen.
\end_layout

\begin_layout Standard
The Cortical Learning Algorithm, or CLA, is the mechanism used by each region
 to learn spatial as well as temporal patterns.
 It has been implemented as part of the Numenta Platform for Intelligent
 Computing (NuPIC), the Python version of which is used in the present work
 for comparison.
\end_layout

\begin_layout Standard
Also part of NuPIC is the so called Online Prediction Framework (OPF), that
 facilitates the setup and running of HTM models for, among other things,
 anomaly detection.
\end_layout

\begin_layout Standard
NuPIC provides yet another utility, called the 
\emph on
swarming
\emph default
 module that, based on a data set, tries to find the parameters that will
 allow an HTM model to perform certain task (e.g.
 prediction or anomaly detection) optimally.
\end_layout

\begin_layout Subsection
MGNG-AD parameter's influence on detection
\end_layout

\begin_layout Standard
One could divide the parameters in two broad categories: parameters related
 to representational capabilities, and parameters related to learning speed.
 There is no stark divide between the two, since parameters that impact
 learning will also influence representational power and vice versa, but
 it is nonetheless useful to think of them that way.
\end_layout

\begin_layout Standard
\begin_inset Formula $\alpha$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

 have influence over how important the temporal context is, thus they are
 related to the representational capabilities; bigger values of either one
 give more importance to past inputs, lower values to the present input.
\end_layout

\begin_layout Standard
\begin_inset Formula $\theta$
\end_inset

 controls how many different patterns the model can represent, hence it
 is also a representational capabilities related parameter; if it is normal
 for the input time series to vary much, then a large 
\begin_inset Formula $\theta$
\end_inset

 should be chosen, more static time series can use a lower value.
\end_layout

\begin_layout Standard
\begin_inset Formula $\delta$
\end_inset

, 
\begin_inset Formula $\eta$
\end_inset

, 
\begin_inset Formula $\epsilon_{w}$
\end_inset

, 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\epsilon_{n}$
\end_inset

 and 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit

\begin_inset Formula $\gamma$
\end_inset

 control how frequently neurons and connections are formed and deleted,
 and how much they are modified, therefore being related to learning speed.
 They should be chosen with care -- if the time series tends to change its
 behavior abruptly, then a faster learning speed should be preferred.
 Make the speed too fast, however, and the model will forget normal patterns
 too quickly, making it, for instance, stop reporting anomalies too early.
\end_layout

\begin_layout Standard
By delaying the insertion of new neurons, and therefore giving time to existing
 neurons to learn to represent the input, 
\begin_inset Formula $\lambda$
\end_inset

 has a strong influence on both learning speed and representational capabilities.
 
\begin_inset Formula $\lambda$
\end_inset

 should be chosen in relation to the chosen 
\begin_inset Formula $\theta$
\end_inset

.
 If 
\begin_inset Formula $\lambda$
\end_inset

 is less or no much bigger than 
\begin_inset Formula $\theta$
\end_inset

, one runs the risk of inserting too many neurons too quickly, without giving
 them time to adapt.
 A value of 
\begin_inset Formula $\lambda$
\end_inset

 far to big will cause the model to have too few neurons, perhaps even less
 than the number needed to sufficiently represent the input.
\end_layout

\begin_layout Subsection
General Setup
\begin_inset CommandInset label
LatexCommand label
name "sub:General_setup"

\end_inset


\end_layout

\begin_layout Standard
Given the amount of parameters needed for a MGNG-AD model 
\begin_inset Formula $D=\left(f,\omega,\mathcal{A},M^{P},M^{Q},j\right)$
\end_inset

, it was decided to initialize the parameters 
\begin_inset Formula $\alpha$
\end_inset

, 
\begin_inset Formula $\beta$
\end_inset

, 
\begin_inset Formula $\gamma$
\end_inset

, 
\begin_inset Formula $\delta$
\end_inset

, 
\begin_inset Formula $\theta$
\end_inset

, 
\begin_inset Formula $\eta$
\end_inset

, 
\begin_inset Formula $\lambda$
\end_inset

, 
\begin_inset Formula $\epsilon_{w}$
\end_inset

 and 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\epsilon_{n}$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 of the models 
\begin_inset Formula $M^{P}$
\end_inset

 and 
\begin_inset Formula $M^{Q}$
\end_inset

 same way.
 Another reason for initializing 
\begin_inset Formula $M^{P}$
\end_inset

 and 
\begin_inset Formula $M^{Q}$
\end_inset

 with the same parameter is that, intuitively, one wants both models to
 achieve the same representation for normal sequences; this would be difficult
 to achieve with different initializations for 
\begin_inset Formula $M^{P}$
\end_inset

 and 
\begin_inset Formula $M^{Q}$
\end_inset

.
\end_layout

\begin_layout Standard
All parameter's values in all experiments were either empirically determined
 as the ones that maximized the F1-Score for experiment, or taken from the
 literature.
 It would be desirable to use an approach similar to the swarming offered
 by NuPIC for finding their optimal settings.
 Such development is, however, outside the scope of the present work.
\end_layout

\begin_layout Standard
The strategy for setting up the HTM models was to feed the swarm module
 with a number of samples from the start of each data-set, in order to find
 the parameters for the HTM, and then start feeding the optimized model
 a sample at a time from the start, using the OPF.
 This will result in an output file that contains the original data-set,
 together with two additional columns, anomaly_score and anomaly_likelihood,
 the last of which shall be used to determine whether a data-point is an
 anomaly.
 The result of the swarming process, namely the parameters for the OPF,
 will be treated as internal parameters of the entire HTM experiment, and
 thus not shown.
\end_layout

\begin_layout Standard
The swarming over a sub-sequence from the start of the time series was done
 in order to simulate a real scenario: upon start, a number of the first
 samples to come from a live data stream are used for swarming, and then
 start feeding the HTM model either from the start or from where the swarm
 left.
\end_layout

\begin_layout Subsection
Evaluation Methodology
\begin_inset CommandInset label
LatexCommand label
name "sub:Evaluation_methodology"

\end_inset


\end_layout

\begin_layout Standard
In order to evaluate the performance of MGNG-AD and HTM-based anomaly detection,
 synthetic time series have been generated.
\end_layout

\begin_layout Standard
Each time series 
\begin_inset Formula $S$
\end_inset

, stored in CSV
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "CSV"
description "Comma Separated Values"

\end_inset

 format, has been fed to an MGNG-AD model and to an HTM-model.
 The anomaly values of each data-point in the data-set generated by both
 models are stored in a different CSV file, together with the data-point's
 time-step, value and annotation.
\end_layout

\begin_layout Standard
This output file is then processed to extract the precision 
\begin_inset Formula $p$
\end_inset

 and recall 
\begin_inset Formula $r$
\end_inset

 values according to the following formulas:
\begin_inset Formula 
\begin{align*}
p & =\frac{\left|true\, positive\right|}{\left|true\, positive\right|+\left|false\, positive\right|}\\
r & =\frac{\left|true\, positive\right|}{\left|true\, positive\right|+\left|false\, negative\right|}
\end{align*}

\end_inset

where a true positive indicates a data-point annotated as an anomaly and
 recognized as an anomaly by the anomaly detector, a false negative indicates
 a data-point annotated as anomaly that was not recognized as such by the
 anomaly detector, and a false positive indicates a data-point that was
 not annotated as an anomaly but that was falsely recognized as one by the
 anomaly detector.
\end_layout

\begin_layout Standard
Based on 
\begin_inset Formula $p$
\end_inset

 and 
\begin_inset Formula $r$
\end_inset

 the F1-Score for 
\begin_inset Formula $S$
\end_inset

 is calculated according to:
\begin_inset Formula 
\begin{align*}
F_{1} & =2\cdot\frac{p\cdot r}{p+r}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
A threshold is set in order to determine whether a data-point is classified
 as an anomaly by the anomaly detector.
 
\end_layout

\begin_layout Standard
When presenting results, precision, recall, F1-Score and the threshold used
 are rounded to four decimal places.
\end_layout

\begin_layout Standard
The thresholds are always applied to the output files' anomaly_likelihood,
 in the HTM case, and anomaly_score, in the MGNG-AD case, columns.
 Given a threshold, a data-point is flagged as anomalous by a model, if
 the corresponding data-point's HTM anomaly_likelihood or MGNG-AD anomaly_score
 is above the threshold.
 In each experiment the thresholds for the models were selected so as to
 maximize each model's F1-Score.
\end_layout

\begin_layout Standard
Worth of notice is that this methodology presents several problems.
 First, the F1-Score was developed to evaluate binary classificators.
 In this case HTM as well as MGNG-AD do not provide an anomaly class for
 each point but rather a degree of abnormality, hence the need of setting
 a threshold.
 However the resulting F1-Score will vary depending on selected threshold,
 as shown in the experiment's results.
\end_layout

\begin_layout Standard
Second, the extremely low amount of anomalies found in real data-sets, together
 with the known high false positive rate common to many unsupervised methods
 (in comparison with supervised ones, see 
\begin_inset CommandInset citation
LatexCommand cite
after "p. 30"
key "Chandola2009"

\end_inset

, 
\begin_inset CommandInset citation
LatexCommand cite
after "p. 17"
key "Patcha2007"

\end_inset

, 
\begin_inset CommandInset citation
LatexCommand cite
after "p. 2"
key "Anstey2005"

\end_inset

), will generally produce low 
\begin_inset Formula $p$
\end_inset

 rates.
\end_layout

\begin_layout Standard
Third, the kind of anomalies MGNG-AD or HTM are able to detect are not necessari
ly the same as the anomalies annotated in real scenarios.
 Take, for instance, the case of an apnea sleep study.
 Even though the apnea is caused by a respiratory phenomenon, the identification
 of an apnea episode can take into consideration other variables
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
see for instance http://www.webmd.com/sleep-disorders/guide/understanding-obstruct
ive-sleep-apnea-syndrome
\end_layout

\end_inset

, variables which are not necessarily available to the anomaly detectors.
\end_layout

\begin_layout Standard
The inadequacy of the F1-Score as measure of MGNG-AD's and HTM's performance
 as anomaly detectors should not be a surprise.
 As pointed out by Aggarwal 
\begin_inset CommandInset citation
LatexCommand cite
after "Ch. 1, Sec. 7"
key "Aggarwal2013"

\end_inset

:
\end_layout

\begin_layout Quote
A key question arises as to how the effectiveness of an outlier detection
 algorithm should be evaluated.
 Unfortunately, this is often a difficult task, because outliers, by definition,
 are rare.
 This means that the ground-truth about which data points are outliers,
 is often not available.
 This is especially true for unsupervised algorithms, because if the ground-trut
h were indeed available, it could have been used to create a more effective
 supervised algorithm.
 In the unsupervised scenario (without ground-truth), it is often the case,
 that no realistic quantitative methods can be used in order to judge the
 effectiveness of the underlying algorithms in a rigorous way.
\end_layout

\begin_layout Standard
In other words, although it is possible to quatitatively evaluate the performanc
e of an unsupervised method at detecting known anomalies, it is not possible
 to perform such evaluation at the task of detecting unknown ones.
\end_layout

\begin_layout Standard
In spite of its flaws, this approach was considered the best quantitative
 option for the model's evaluation.
 Taking all this into consideration, the F1-Scores should not be used to
 judge the methods at the task of detecting any kind of anomalies, but rather
 only at the task of detecting the anomalies presented in the experiments.
\end_layout

\end_body
\end_document
